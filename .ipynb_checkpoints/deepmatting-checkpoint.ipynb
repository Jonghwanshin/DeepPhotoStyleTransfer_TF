{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Photo Transfer\n",
    "this is the tensorflow version of deep photo transfer \n",
    "deepmatting_seg.lua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library import for deep photo transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples/input/in1.png\n",
      "examples/input/in1.png\n",
      "examples/style/tar1.png\n"
     ]
    }
   ],
   "source": [
    "init_image_fn = \"examples/input/in1.png\"\n",
    "content_image_fn = \"examples/input/in1.png\"\n",
    "style_image_fn = \"examples/style/tar1.png\"\n",
    "content_seg_fn =  \"\"\n",
    "style_seg_fn = \"\"\n",
    "\n",
    "content_layer =\"\"\n",
    "style_layer = \"\"\n",
    "\n",
    "index = 0;\n",
    "\n",
    "# Local Affine parameters\n",
    "lambda_p = 1e+4\n",
    "patch = 3\n",
    "eps = 1e-7\n",
    "\n",
    "# reconstruct best local affine using joint bilateral smoothing\n",
    "f_radius = 7\n",
    "f_edge = 0.05\n",
    "\n",
    "# output options\n",
    "print_iter = 1\n",
    "save_iter = 100\n",
    "output_iamge = \"out.png\"\n",
    "index = 1\n",
    "serial = \"serial_example\"\n",
    "\n",
    "# other options\n",
    "#proto_file = 0\n",
    "#model_file = 0\n",
    "#backend\n",
    "#cudnn_autotune\n",
    "seed = 612\n",
    "\n",
    "print(init_image_fn)\n",
    "print(content_image_fn)\n",
    "print(style_image_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Main Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Load\n",
    "- Init, Content and Style Image Load\n",
    "- preprocess image for these images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples/input/in1.png\n",
      "examples/input/in1.png\n",
      "examples/style/tar1.png\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-ca2be5cc58b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[1;31m# Preprocess image for caffe model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0minit_image_caffe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mcontent_image_caffe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mstyle_image_caffe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstyle_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-ca2be5cc58b7>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmean_pixel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m103.939\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m116.77\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m123.68\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mperm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mmean_pixel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_pixel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpandAs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmean_pixel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "print(init_image_fn)\n",
    "print(content_image_fn)\n",
    "print(style_image_fn)\n",
    "\n",
    "init_image = ndimage.imread(init_image_fn).astype(float)\n",
    "content_image = ndimage.imread(content_image_fn).astype(float)\n",
    "style_image = ndimage.imread(style_image_fn).astype(float)\n",
    "\n",
    "# Setting up c,h,w for each images\n",
    "c,h,w = content_image.shape[2], content_image.shape[1], content_image.shape[0]\n",
    "c2, h2, w2 = style_image.shape[2], style_image.shape[0], style_image.shape[1]\n",
    "\n",
    "# preprocess an image before passing it to a Caffe model.\n",
    "# We need to rescale from [0,1] to [0,255] convert from RGB to BGR\n",
    "# and subtract the mean pixel\n",
    "def preprocess(image):\n",
    "# define tensor with torch.DoubleTensor({103.939, 116.77, 123.68})\n",
    "    mean_pixel = tf.constant([103.939, 116.77, 123.68])\n",
    "    perm = tf.constant([3,2,1])\n",
    "    img = image.index(1, perm).mul(256.0)\n",
    "    mean_pixel = mean_pixel.view(3,1,1).expandAs(img)\n",
    "    img.add(-1,mean_pixel)\n",
    "    return img\n",
    "\n",
    "# Preprocess image for caffe model\n",
    "init_image_caffe = preprocess(init_image)\n",
    "content_image_caffe = preprocess(content_image)\n",
    "style_image_caffe = preprocess(style_image)\n",
    "\n",
    "# layers\n",
    "content_layers = content_layer.split(\",\")\n",
    "style_layers = style_layer.split(\",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation Images\n",
    "- load content_segmentation, 3\n",
    "- content segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get segmentation images\n",
    "content_seg = ndlimage.imread(content_seg_fn)\n",
    "content_seg = tf.image.resize_images(content_seg, [h,w], ResizeMethod.BILINEAR)\n",
    "style_seg = ndlimage.imread(style_seg_fn)\n",
    "style_seg = tf.image.resize_images(style_seg, [h2,w2], ResizeMethod.BILINEAR)\n",
    "\n",
    "# define color codes\n",
    "color_codes = {'blue', 'green', 'black', 'white', 'red', 'yellow', 'grey', 'lightblue', 'purple'}\n",
    "color_content_masks, color_style_masks = {}, {}\n",
    "for j=1 in range(color_codes)\n",
    "    content_mask_j = ExtractMask(content_seg_codes[j])\n",
    "    style_mask_j = ExtractMask(style_seg_codes[j])\n",
    "    numpy.append(color_content_masks, content_mask_j)\n",
    "    numpy.append(colort_style_masks,style_mask_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-72-e3902ad44d44>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-72-e3902ad44d44>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    tv_mod = nn.TvLoss(param)\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "content_losses, style_losses = {}, {}\n",
    "next_content_idx, next_style_idx = 1, 1\n",
    "net = tf.Graph()\n",
    "\n",
    "# Load VGG-19 Network\n",
    "cnn\n",
    "\n",
    "#  Load Matting Laplacian\n",
    "CSR_fn = 'gen_laplacian/Input_Laplacian_'+toString(patch)+'x'+toString(patch)+'_1e-7_CSR_'+toString(index)+\".mat\"\n",
    "print('loading matting laplacian',CSR_fn)\n",
    "#matlab library for tensorflow\n",
    "\n",
    "\n",
    "#  TvLoss Setup\n",
    "if tv_weight > 0:\n",
    "    tv_mod#nn.TvLoss(param)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data\n",
    "    tf_content_image = tf.placeholder(tf.float32, shape = (c,w,h))\n",
    "    tf_style_image = tf.placeholder(tf.float, shape = (c2,w2,h2))\n",
    "    \n",
    "    # Variables\n",
    "    \n",
    "    \n",
    "    # Model\n",
    "    # How does it look like? VGGNet + some layers for style transfer\n",
    "        \n",
    "    # Training computation\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean() #style loss + content loss\n",
    "    \n",
    "    # Predictions for the training, validation and test data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GramMatrix()\n",
    "#net = nn.Sequential\n",
    "#net:add(nn.View(-1):setNumInputDims(2))\n",
    "# concat = nn.ConcatTable()\n",
    "# concat:add(nn.Identify())\n",
    "# concat:add(nn.Identify())\n",
    "# net:add(concat)\n",
    "# net:add(nn.MM(false, true))\n",
    "# return net \n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style_loss = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ExtractMask(seg,color)\n",
    "    mask = nil\n",
    "    if color == 'green' then\n",
    "        mask = \n",
    "        mask = \n",
    "        mask = \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run\n",
    "    print('Initialized')\n",
    "    for step in range(num_iter)\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
